{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b928ba",
   "metadata": {},
   "source": [
    "## Data scrupping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0405e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "class BatchScraper:\n",
    "    def __init__(self, base_url=\"https://www.deeplearning.ai/the-batch/\"):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/125.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": (\n",
    "        \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "        \"image/avif,image/webp,image/apng,*/*;q=0.8\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "        })\n",
    "\n",
    "    def scrape_articles(self, max_articles=400):\n",
    "        \"\"\"Scrape articles from The Batch\"\"\"\n",
    "        all_articles = []\n",
    "        issue_links = self.get_issue_links()\n",
    "\n",
    "        for issue_url in issue_links:\n",
    "            articles = self.scrape_single_article(issue_url)\n",
    "            for article in articles:\n",
    "                if len(all_articles) >= max_articles:\n",
    "                    return all_articles\n",
    "                all_articles.append(article)\n",
    "            time.sleep(1)\n",
    "\n",
    "        return all_articles\n",
    "        \"\"\"Get all issue URLs\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(self.base_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.select(\"a[href*='/the-batch/']\")\n",
    "            issue_links = set()\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href.startswith('/the-batch/') and href.count('/') > 2:                        \n",
    "                    full_url = urljoin(\"https://www.deeplearning.ai\", href.rstrip('/'))\n",
    "                    issue_links.add(full_url)\n",
    "\n",
    "            return list(issue_links)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving issue links: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_issue_urls(self):\n",
    "        issue_links = set()\n",
    "        links_on_links=set()\n",
    "        page_num = 1\n",
    "        \n",
    "        while True:\n",
    "            if page_num == 1:\n",
    "                url = \"https://www.deeplearning.ai/the-batch/\"  # –ø–µ—Ä—à–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –±–µ–∑ /page/1/\n",
    "            else:\n",
    "                url = f\"{self.base_url}/page/{page_num}/\"\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                if response.status_code == 404:\n",
    "                    display(\"Page not found, stopping scraping.\"+url)\n",
    "                    break  # –∑–∞–∫—ñ–Ω—á–∏—Ç–∏ —Ü–∏–∫–ª, —è–∫—â–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                links = soup.select(\"a[href*='/the-batch/']\")\n",
    "\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    if href.startswith('/the-batch/') and href.count('/') > 2 and \"the-batch/page/\" not in href :\n",
    "                        if \"/tag/\" in href:\n",
    "                            links_on_links.add(urljoin(\"https://www.deeplearning.ai\", href.rstrip('/')))\n",
    "                        else:\n",
    "                            full_url = urljoin(\"https://www.deeplearning.ai\", href.rstrip('/'))\n",
    "                            issue_links.add(full_url)\n",
    "\n",
    "                page_num += 1\n",
    "                time.sleep(1)  # –ø–∞—É–∑–∞ –≤ 1 —Å–µ–∫—É–Ω–¥—É –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error retrieving issue links from {url}: {e}\")\n",
    "                break\n",
    "        for url in links_on_links:\n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                if response.status_code != 200:\n",
    "                    display(f\"Page not found, skipping {url}\")\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                links = soup.select(\"a[href*='/the-batch/']\")\n",
    "\n",
    "                for link in links:\n",
    "                    href = link.get('href')\n",
    "                    if href.startswith('/the-batch/') and href.count('/') > 2 and \"the-batch/page/\" not in href and \"/tag/\" not in href:\n",
    "                        full_url = urljoin(\"https://www.deeplearning.ai\", href.rstrip('/'))\n",
    "                        issue_links.add(full_url)\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing links_on_links url {url}: {e}\")\n",
    "\n",
    "        json.dump(list(issue_links), open(\"issue_links.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=4)\n",
    "        json.dump(list(links_on_links), open(\"links_on_links.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=4)\n",
    "        return list(issue_links)\n",
    "    \n",
    "    def scrape_issue_article(self, url):\n",
    "        \"\"\"Scrape all articles from a single The Batch issue page\"\"\"\n",
    "        \n",
    "        articles = []\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            content_blocks = soup.find_all(['h1',\"h2\",'p', 'img'])\n",
    "            current_article = {}\n",
    "            article_started = False\n",
    "            found_news_heading = False\n",
    "\n",
    "            last_img = None  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–µ—Ä–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º\n",
    "\n",
    "            for tag in content_blocks:\n",
    "                if (tag.name == 'h2' or tag.name ==\"h1\" ) and \"news\" in tag.get_text(strip=True).lower():\n",
    "                    found_news_heading = True  # üëà –¥–æ–∑–≤–æ–ª—è—î–º–æ –ø–æ—á–∞—Ç–∏ –æ–±—Ä–æ–±–∫—É –ø—ñ—Å–ª—è —Ü—å–æ–≥–æ\n",
    "                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ \"news\"\n",
    "                elif not found_news_heading:\n",
    "                    continue  # ‚õî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤—Å–µ –¥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ \"news\"\n",
    "                if tag.name == 'img':\n",
    "                    img_src = tag.get('src')\n",
    "                    if img_src == \"/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&w=640&q=75\":\n",
    "                        # Skip this article by breaking out of the loop\n",
    "                        articles = []\n",
    "                        continue\n",
    "                    if img_src and not img_src.startswith(\"data:\"):\n",
    "                        last_img = {\n",
    "                            'url': urljoin(url, img_src),\n",
    "                            'alt': tag.get('alt', ''),\n",
    "                            'caption': self.extract_image_caption(tag)\n",
    "                        }\n",
    "\n",
    "                elif tag.name in ['h2', 'h1'] :\n",
    "                    if article_started and current_article:\n",
    "                        current_article['url'] = url\n",
    "                        pub_date_elem = soup.find('div', class_='mt-1 text-slate-600 text-base text-sm')\n",
    "                        print(f\"pub_date_elem: {pub_date_elem}\")\n",
    "                        if pub_date_elem and pub_date_elem.get_text(strip=True):\n",
    "                            current_article['publication_date'] = pub_date_elem.get_text(strip=True)\n",
    "                        else:\n",
    "                            current_article['publication_date'] = str(datetime.now())\n",
    "                        current_article['scraped_at'] = str(datetime.now())\n",
    "                        articles.append(current_article)\n",
    "                        current_article = {}\n",
    "\n",
    "                    current_article['title'] = tag.get_text(strip=True)\n",
    "                    current_article['content'] = \"\"\n",
    "                    current_article['images'] = []\n",
    "\n",
    "                    if last_img:\n",
    "                        current_article['images'].append(last_img)\n",
    "                        last_img = None  # –û—á–∏—Å—Ç–∏—Ç–∏, —â–æ–± –Ω–µ –¥—É–±–ª—é–≤–∞—Ç–∏ –≤ –Ω–∞—Å—Ç—É–ø–Ω—ñ–π —Å—Ç–∞—Ç—Ç—ñ\n",
    "\n",
    "                    article_started = True\n",
    "\n",
    "                elif tag.name == 'p' and article_started:\n",
    "                    current_article['content'] += tag.get_text(strip=True) + \" \"\n",
    "\n",
    "                elif tag.name == 'img' and article_started:\n",
    "                    img_src = tag.get('src')\n",
    "                    if img_src and not img_src.startswith(\"data:\"):\n",
    "                        img_url = urljoin(url, img_src)\n",
    "                        current_article['images'].append({\n",
    "                            'url': img_url,\n",
    "                            'alt': tag.get('alt', ''),\n",
    "                            'caption': self.extract_image_caption(tag)\n",
    "                        })\n",
    "\n",
    "            if current_article and article_started:\n",
    "                current_article['url'] = url\n",
    "                pub_date_elem = soup.find('div', class_='mt-1 text-slate-600 text-base text-sm')\n",
    "                current_article['publication_date'] = pub_date_elem.get_text(strip=True) or str(datetime.now())\n",
    "                current_article['scraped_at'] = str(datetime.now())\n",
    "                articles.append(current_article)\n",
    "\n",
    "            return articles\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_simple_article(self, url):\n",
    "        \"\"\"Scrape a single The Batch article page (title, date, paragraphs, main image)\"\"\"\n",
    "        article = {}\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            if response.status_code != 200:\n",
    "                logging.error(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "                return []\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "            title_tag = soup.find('h1')\n",
    "            article['title'] = title_tag.get_text(strip=True) if title_tag else 'No Title'\n",
    "\n",
    "            # –ü—É–±–ª—ñ–∫–∞—Ü—ñ—è\n",
    "            date_tags = soup.find_all('div', class_=\"inline-flex px-3 py-1 text-sm font-normal transition-colors rounded-md bg-slate-200 hover:bg-slate-300 text-slate-500\")\n",
    "            for tag in date_tags:\n",
    "                text = tag.get_text(strip=True)\n",
    "                if re.search(r'\\d', text):\n",
    "                    article['publication_date'] = text\n",
    "                    break  # –∑—É–ø–∏–Ω–∏–º–æ—Å—è –Ω–∞ –ø–µ—Ä—à–æ–º—É –≤–∞–ª—ñ–¥–Ω–æ–º—É\n",
    "                else:\n",
    "                    article['publication_date'] = None\n",
    "\n",
    "            # –û—Å–Ω–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "            main_img_tags = soup.find_all('img', attrs={'alt': True, 'srcset': True})\n",
    "            for main_img_tag in main_img_tags:\n",
    "                if \"batch-logo.\"  not in main_img_tag.get('srcset', '') :\n",
    "                    srcset = main_img_tag.get('srcset', '')\n",
    "                    \n",
    "                    # –í–∏–±–∏—Ä–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –Ω–∞–π–≤–∏—â–æ—é —Ä–æ–∑–¥—ñ–ª—å–Ω—ñ—Å—Ç—é\n",
    "                    last_img = srcset.split(',')[-1].strip().split(' ')[0]\n",
    "                    article['image'] = {\n",
    "                        'url': urljoin(url, last_img),\n",
    "                        'alt': main_img_tag.get('alt', '')\n",
    "                    }\n",
    "                    break\n",
    "                else:\n",
    "                    article['image'] = None\n",
    "\n",
    "            # –ö–æ–Ω—Ç–µ–Ω—Ç\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "            article['content'] = content\n",
    "\n",
    "            # –ú–µ—Ç–∞–¥–∞–Ω—ñ\n",
    "            article['url'] = url\n",
    "            article['scraped_at'] = str(datetime.now())\n",
    "\n",
    "            # –ó–±–µ—Ä–µ–≥—Ç–∏ —É —Ñ–∞–π–ª\n",
    "            with open(\"articles2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump([article], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            return [article]\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "        \"\"\"Extract publication date from soup\"\"\"\n",
    "        date_elem = soup.find('time')\n",
    "        return date_elem.get('datetime') if date_elem else None\n",
    "\n",
    "    def download_images(self, articles, img_dir='data/images'):\n",
    "        \"\"\"Download images from articles\"\"\"\n",
    "        os.makedirs(img_dir, exist_ok=True)\n",
    "        \n",
    "        for article in articles:\n",
    "            images = article.get('images', [])\n",
    "            if not images:\n",
    "                continue\n",
    "            for i, img_data in enumerate(images):\n",
    "                try:\n",
    "                    response = self.session.get(img_data['url'])\n",
    "                    if response.status_code == 200:\n",
    "                        parsed_url = urlparse(img_data['url'])\n",
    "                        filename = f\"{article['title'][:50]}_{i}_{os.path.basename(parsed_url.path)}\"\n",
    "                        filename = \"\".join(c for c in filename if c.isalnum() or c in '.-_')\n",
    "                        filepath = os.path.join(img_dir, filename)\n",
    "                        with open(filepath, 'wb') as f:\n",
    "                            f.write(response.content)\n",
    "                        img_data['local_path'] = filepath\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error downloading image {img_data['url']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ed905",
   "metadata": {},
   "source": [
    "get links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c12d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page not found, stopping scraping.https://www.deeplearning.ai/the-batch/page/23/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scraper= BatchScraper()\n",
    "a=scraper.get_all_issue_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c73420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scraper= BatchScraper()\n",
    "b=scraper.scrape_issue_article(\"https://www.deeplearning.ai/the-batch/issue-4\")\n",
    "b=scraper.scrape_simple_article(\"https://www.deeplearning.ai/the-batch/build-career-part-6\")\n",
    "c=scraper.scrape_simple_article(\"https://www.deeplearning.ai/the-batch/stability-ai-launches-stable-audio-a-text-to-music-generator-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e4222ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 6, 2021</div>\n",
      "[50/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-233\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 26, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 29, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 29, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 29, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 29, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 29, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 14, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 14, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 14, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 14, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 14, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2020</div>\n",
      "[100/2079] Scraping: https://www.deeplearning.ai/the-batch/do-oil-and-algorithms-mix\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 18, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 18, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 18, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 18, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 11, 2023</div>\n",
      "[150/2079] Scraping: https://www.deeplearning.ai/the-batch/clothes-make-the-model\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 30, 2022</div>\n",
      "[200/2079] Scraping: https://www.deeplearning.ai/the-batch/autonomous-trucks-hit-the-gas\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 27, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 7, 2024</div>\n",
      "[250/2079] Scraping: https://www.deeplearning.ai/the-batch/reinforcement-learning-plus-transformers-equals-efficiency\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 2, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 2, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 2, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 2, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 4, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 4, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 4, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 4, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 19, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 12, 2023</div>\n",
      "[300/2079] Scraping: https://www.deeplearning.ai/the-batch/excitement-recognition\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 12, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 22, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 22, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 22, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 22, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 10, 2021</div>\n",
      "[350/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-72\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2019</div>\n",
      "[400/2079] Scraping: https://www.deeplearning.ai/the-batch/chess-the-next-move\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 26, 2025</div>\n",
      "[450/2079] Scraping: https://www.deeplearning.ai/the-batch/the-dawning-age-of-agents\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 5, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 5, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 5, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 5, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 30, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 30, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 30, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 30, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 15, 2023</div>\n",
      "[500/2079] Scraping: https://www.deeplearning.ai/the-batch/confronting-the-fear-of-a-global-chip-shortage-in-2022\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 28, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 28, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 28, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 28, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 20, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 24, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2019</div>\n",
      "[550/2079] Scraping: https://www.deeplearning.ai/the-batch/project-relate\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 21, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 8, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 29, 2020</div>\n",
      "[600/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-83\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 2, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 7, 2021</div>\n",
      "[650/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-34\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 8, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 3, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2024</div>\n",
      "[700/2079] Scraping: https://www.deeplearning.ai/the-batch/openai-will-stop-serving-users-in-china-and-other-nations-of-concern-to-the-u-s-government-as-soon-as-next-week-whats-new-open-ai-notified-users-in-china-they-would-lose-api-access-on-j\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 14, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 14, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 30, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 9, 2020</div>\n",
      "[750/2079] Scraping: https://www.deeplearning.ai/the-batch/why-ai-projects-fail-part-4-small-data\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 4, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 28, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 13, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 5, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 5, 2024</div>\n",
      "[800/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-145\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 18, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 18, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 18, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 18, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 18, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 1, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 9, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 5, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 14, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 26, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 26, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 26, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 26, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 26, 2021</div>\n",
      "[850/2079] Scraping: https://www.deeplearning.ai/the-batch/how-to-become-a-multilingual-coder\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 9, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 13, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 26, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 31, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 20, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 10, 2024</div>\n",
      "[900/2079] Scraping: https://www.deeplearning.ai/the-batch/closing-in-on-cancer\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 18, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 15, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 2, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 4, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 4, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 4, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 4, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 4, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 7, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 15, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 23, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 20, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 23, 2021</div>\n",
      "[950/2079] Scraping: https://www.deeplearning.ai/the-batch/text-to-speech-in-parallel\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 28, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 4, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 4, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 4, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 4, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 16, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 16, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 16, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 16, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 28, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 24, 2019</div>\n",
      "[1000/2079] Scraping: https://www.deeplearning.ai/the-batch/how-machina-labs-uses-ai-to-automate-metal-fabrication\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 11, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 5, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 15, 2023</div>\n",
      "[1050/2079] Scraping: https://www.deeplearning.ai/the-batch/ai-transformed\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 25, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 21, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 1, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 1, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 1, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 1, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 1, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 12, 2020</div>\n",
      "[1100/2079] Scraping: https://www.deeplearning.ai/the-batch/making-gans-more-inclusive\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 3, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 27, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 19, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 19, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 19, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 19, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 19, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 2, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 22, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 22, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 22, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 22, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 22, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 19, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 9, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 9, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 9, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 9, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 3, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 3, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 30, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 17, 2021</div>\n",
      "[1150/2079] Scraping: https://www.deeplearning.ai/the-batch/meta-updates-llama-models-with-vision-language-edge-sizes-and-agentic-apis\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 4, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 25, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 18, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 11, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 11, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 11, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 11, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 11, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 31, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 7, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 7, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 7, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 7, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 5, 2019</div>\n",
      "[1200/2079] Scraping: https://www.deeplearning.ai/the-batch/copilot-ai-tool-may-cause-programmers-to-write-buggy-code\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 12, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 21, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 21, 2021</div>\n",
      "[1250/2079] Scraping: https://www.deeplearning.ai/the-batch/motion-mapper\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 21, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 21, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 21, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 21, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 21, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 6, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 30, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 15, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 15, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 15, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 15, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 15, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 30, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 30, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 30, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 30, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 30, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 2, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 2, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 2, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 2, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 2, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 29, 2020</div>\n",
      "[1300/2079] Scraping: https://www.deeplearning.ai/the-batch/richer-video-representations\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 17, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 9, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 22, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 7, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 21, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 21, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 21, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 21, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 17, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 27, 2020</div>\n",
      "[1350/2079] Scraping: https://www.deeplearning.ai/the-batch/protein-shapes-revealed\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 14, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 10, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 6, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 23, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 23, 2022</div>\n",
      "[1400/2079] Scraping: https://www.deeplearning.ai/the-batch/watermarking-is-a-no-go\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 4, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 3, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 3, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 3, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 3, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 3, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 2, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 7, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 7, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 16, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 16, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 16, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 16, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 16, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 8, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 8, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 8, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 8, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 8, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 1, 2023</div>\n",
      "[1450/2079] Scraping: https://www.deeplearning.ai/the-batch/ai-matches-humans-in-breast-cancer-diagnosis\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 14, 2024</div>\n",
      "[1500/2079] Scraping: https://www.deeplearning.ai/the-batch/a-privacy-threat-revealed\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 21, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 10, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 7, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 6, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 17, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 1, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 10, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 10, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 10, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 10, 2022</div>\n",
      "[1550/2079] Scraping: https://www.deeplearning.ai/the-batch/fake-aim\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 15, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 26, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 23, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 23, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 23, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 23, 2025</div>\n",
      "[1600/2079] Scraping: https://www.deeplearning.ai/the-batch/the-2024-paris-olympics-may-have-ai-surveillance\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 1, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 6, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 31, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 5, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 16, 2020</div>\n",
      "[1650/2079] Scraping: https://www.deeplearning.ai/the-batch/facing-down-deepfakes\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 10, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 4, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 6, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 27, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 16, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 25, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 25, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 25, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 25, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2023</div>\n",
      "[1700/2079] Scraping: https://www.deeplearning.ai/the-batch/data-points-issue-244\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 18, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 31, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 31, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 31, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 31, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Apr 20, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 15, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 22, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 22, 2023</div>\n",
      "[1750/2079] Scraping: https://www.deeplearning.ai/the-batch/new-supercomputer-on-the-block\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 11, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 11, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 11, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 11, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 22, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 13, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 26, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 27, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 26, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 26, 2025</div>\n",
      "[1800/2079] Scraping: https://www.deeplearning.ai/the-batch/personal-trainer\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 19, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 19, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 19, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 19, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 19, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 12, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 13, 2023</div>\n",
      "[1850/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-x\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 19, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 17, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 16, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 13, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 13, 2024</div>\n",
      "[1900/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-159\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 24, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 2, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 22, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 25, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 25, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 25, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 25, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 25, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 11, 2020</div>\n",
      "[1950/2079] Scraping: https://www.deeplearning.ai/the-batch/the-neurips-data-centric-ai-workshop-was-amazing\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">May 29, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 24, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Dec 8, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Aug 14, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 20, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 11, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 11, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 11, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 11, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jun 11, 2025</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 4, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 9, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jan 8, 2020</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Feb 24, 2021</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2024</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 23, 2024</div>\n",
      "[2000/2079] Scraping: https://www.deeplearning.ai/the-batch/issue-219\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Oct 18, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Mar 29, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 28, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 28, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 28, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 28, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 28, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 16, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Nov 16, 2022</div>\n",
      "[2050/2079] Scraping: https://www.deeplearning.ai/the-batch/competitive-coder\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 14, 2022</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Sep 11, 2019</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "pub_date_elem: <div class=\"mt-1 text-slate-600 text-base text-sm\">Jul 19, 2023</div>\n",
      "‚úÖ Done scraping. Total articles: 3176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Optional: set up logging to file\n",
    "logging.basicConfig(filename=\"scrape_errors.log\", level=logging.ERROR, format='%(asctime)s %(message)s')\n",
    "\n",
    "results = []\n",
    "failed_urls = []\n",
    "\n",
    "# Load the URLs from JSON file\n",
    "with open(\"issue_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    issue_urls = json.load(f)\n",
    "\n",
    "for idx, url in enumerate(issue_urls, start=1):\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"[{idx}/{len(issue_urls)}] Scraping: {url}\")\n",
    "    \n",
    "    try:\n",
    "        if \"the-batch/issue-\" in url:\n",
    "            articles = scraper.scrape_issue_article(url)\n",
    "        else:\n",
    "            articles = scraper.scrape_simple_article(url)\n",
    "        results.extend(articles)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "        print(f\"‚ùå Failed: {url}\")\n",
    "        failed_urls.append(url)\n",
    "\n",
    "    # Respectful crawling delay\n",
    "    time.sleep(1.5)  # adjust to 2‚Äì3 seconds if you're hitting rate limits\n",
    "\n",
    "# Save all successfully scraped articles\n",
    "with open(\"scraped_articles.json\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump(results, out_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save failed URLs for retry/debug\n",
    "if failed_urls:\n",
    "    with open(\"failed_urls.json\", \"w\", encoding=\"utf-8\") as failed_f:\n",
    "        json.dump(failed_urls, failed_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Done scraping. Total articles:\", len(results))\n",
    "if failed_urls:\n",
    "    print(\"‚ö†Ô∏è Some URLs failed. Check failed_urls.json and scrape_errors.log.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69282b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error downloading image https://cdn2.hubspot.net/hub/5871640/hubfs/ISOMORPHIC.gif?upscale=true&width=1200&upscale=true&name=ISOMORPHIC.gif: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "ERROR:root:Error downloading image https://info.deeplearning.ai/hs-fs/hubfs/ezgif.com-gif-maker%20-%202021-06-15T133323.218.gif?width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-06-15T133323.218.gif: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "ERROR:root:Error downloading image https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-05-19T123452.995.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-19T123452.995.gif: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "ERROR:root:Error downloading image https://info.deeplearning.ai/hs-fs/hubfs/CHIPS.gif?width=1200&upscale=true&name=CHIPS.gif: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "ERROR:root:Error downloading image https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-10-12T111718.458.gif?upscale=true&width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-10-12T111718.458.gif: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "scraper = BatchScraper()\n",
    "with open(\"scraped_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_articles = json.load(f)\n",
    "scraper.download_images(scraped_articles, img_dir='imgages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b4053",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627e250",
   "metadata": {},
   "source": [
    "open ai clip vit 32 (little bit week and can only proceed 77 tocken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0f26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text from HTML, emojis, extra spaces, and known noise\"\"\"\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^<]+?>', '', text)\n",
    "        # Remove known repetitive marketing phrases\n",
    "        text = re.sub(r'‚ú®\\s*New course!', '', text,flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def get_clip_text_embedding(self, text):\n",
    "        \"\"\"Get CLIP text embedding for a given text (max 77 tokens)\"\"\"\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**inputs)\n",
    "        return text_features.numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text_with_title(article_json, chunk_size=77, overlap=8, tokenizer=None):\n",
    "        title = article_json.get(\"title\", \"\")\n",
    "        content = article_json.get(\"content\", \"\").strip()\n",
    "\n",
    "        content_tokens = tokenizer.encode(content, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(content_tokens):\n",
    "            end = min(start + chunk_size, len(content_tokens))\n",
    "            chunk_tokens = content_tokens[start:end]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # –î–æ–¥–∞—î–º–æ title\n",
    "            full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "\n",
    "            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ —É —Ü—å–æ–º—É –æ–±'—î–¥–Ω–∞–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ\n",
    "            tokenized_full = tokenizer(full_chunk, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "            input_ids = tokenized_full['input_ids'][0]\n",
    "\n",
    "            # –Ø–∫—â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–∏—â—É—î 77 –Ω–∞–≤—ñ—Ç—å –ø—ñ—Å–ª—è –æ–±—Ä—ñ–∑–∫–∏ ‚Äî —Å–∫–æ—Ä–æ—Ç–∏–º–æ chunk_text\n",
    "            while len(input_ids) > 77 and len(chunk_tokens) > 5:\n",
    "                chunk_tokens = chunk_tokens[:-1]\n",
    "                chunk_text = tokenizer.decode(chunk_tokens)\n",
    "                full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "                input_ids = tokenizer(full_chunk, return_tensors=\"pt\")['input_ids'][0]\n",
    "\n",
    "            chunks.append(full_chunk)\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image and extract features\"\"\"\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            # Resize if too large\n",
    "            if image.size[0] > 1024 or image.size[1] > 1024:\n",
    "                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Extract CLIP features\n",
    "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.get_image_features(**inputs)\n",
    "                \n",
    "            return image_features.numpy()\n",
    "          \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image}: {e}\")\n",
    "            return None\n",
    "    def download_image_from_url(self,url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {e}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f357cb",
   "metadata": {},
   "source": [
    "more powerful  laion2B-s32B-b79K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee867c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text from HTML, emojis, extra spaces, and known noise\"\"\"\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^<]+?>', '', text)\n",
    "        # Remove known repetitive marketing phrases\n",
    "        text = re.sub(r'‚ú®\\s*New course!', '', text,flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def get_clip_text_embedding(self, text):\n",
    "        \"\"\"Get CLIP text embedding for a given text (max 77 tokens)\"\"\"\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**inputs)\n",
    "        return text_features.numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text_with_title(article_json, chunk_size=256, overlap=16, tokenizer=None):\n",
    "        title = article_json.get(\"title\", \"\")\n",
    "        content = article_json.get(\"content\", \"\").strip()\n",
    "\n",
    "        content_tokens = tokenizer.encode(content, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(content_tokens):\n",
    "            end = min(start + chunk_size, len(content_tokens))\n",
    "            chunk_tokens = content_tokens[start:end]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # –î–æ–¥–∞—î–º–æ title\n",
    "            full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "\n",
    "            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ —É —Ü—å–æ–º—É –æ–±'—î–¥–Ω–∞–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ\n",
    "            tokenized_full = tokenizer(full_chunk, return_tensors=\"pt\", truncation=True, max_length=77)\n",
    "            input_ids = tokenized_full['input_ids'][0]\n",
    "\n",
    "            # –Ø–∫—â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–∏—â—É—î 77 –Ω–∞–≤—ñ—Ç—å –ø—ñ—Å–ª—è –æ–±—Ä—ñ–∑–∫–∏ ‚Äî —Å–∫–æ—Ä–æ—Ç–∏–º–æ chunk_text\n",
    "            while len(input_ids) > 77 and len(chunk_tokens) > 5:\n",
    "                chunk_tokens = chunk_tokens[:-1]\n",
    "                chunk_text = tokenizer.decode(chunk_tokens)\n",
    "                full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "                input_ids = tokenizer(full_chunk, return_tensors=\"pt\")['input_ids'][0]\n",
    "\n",
    "            chunks.append(full_chunk)\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image and extract features\"\"\"\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            # Resize if too large\n",
    "            if image.size[0] > 1024 or image.size[1] > 1024:\n",
    "                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Extract CLIP features\n",
    "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.get_image_features(**inputs)\n",
    "                \n",
    "            return image_features.numpy()\n",
    "          \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image}: {e}\")\n",
    "            return None\n",
    "    def download_image_from_url(self,url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {e}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46757f91",
   "metadata": {},
   "source": [
    "middle by power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa123a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\" )\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\" )\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text from HTML, emojis, extra spaces, and known noise\"\"\"\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^<]+?>', '', text)\n",
    "        # Remove known repetitive marketing phrases\n",
    "        text = re.sub(r'‚ú®\\s*New course!', '', text,flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def get_clip_text_embedding(self, text):\n",
    "        \"\"\"Get CLIP text embedding for a given text (max 77 tokens)\"\"\"\n",
    "        inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**inputs)\n",
    "        return text_features.numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def chunk_text_with_title(article_json, chunk_size=256, overlap=32, tokenizer=None):\n",
    "        title = article_json.get(\"title\", \"\")\n",
    "        content = article_json.get(\"content\", \"\").strip()\n",
    "\n",
    "        content_tokens = tokenizer.encode(content, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(content_tokens):\n",
    "            end = min(start + chunk_size, len(content_tokens))\n",
    "            chunk_tokens = content_tokens[start:end]\n",
    "            chunk_text = tokenizer.decode(chunk_tokens)\n",
    "\n",
    "            # –î–æ–¥–∞—î–º–æ title\n",
    "            full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "\n",
    "            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ —É —Ü—å–æ–º—É –æ–±'—î–¥–Ω–∞–Ω–æ–º—É —Ç–µ–∫—Å—Ç—ñ\n",
    "            tokenized_full = tokenizer(full_chunk, return_tensors=\"pt\", truncation=True)\n",
    "            input_ids = tokenized_full['input_ids'][0]\n",
    "\n",
    "            # –Ø–∫—â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–∏—â—É—î 77 –Ω–∞–≤—ñ—Ç—å –ø—ñ—Å–ª—è –æ–±—Ä—ñ–∑–∫–∏ ‚Äî —Å–∫–æ—Ä–æ—Ç–∏–º–æ chunk_text\n",
    "            while len(input_ids) > 77 and len(chunk_tokens) > 5:\n",
    "                chunk_tokens = chunk_tokens[:-1]\n",
    "                chunk_text = tokenizer.decode(chunk_tokens)\n",
    "                full_chunk = f\"Title: {title}\\nContent: {chunk_text}\"\n",
    "                input_ids = tokenizer(full_chunk, return_tensors=\"pt\")['input_ids'][0]\n",
    "\n",
    "            chunks.append(full_chunk)\n",
    "            start += chunk_size - overlap\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\" )\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\" )\n",
    "    \n",
    "    def process_image(self, image):\n",
    "        \"\"\"Process image and extract features\"\"\"\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            # Resize if too large\n",
    "            if image.size[0] > 1024 or image.size[1] > 1024:\n",
    "                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Extract CLIP features\n",
    "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.get_image_features(**inputs)\n",
    "                \n",
    "            return image_features.numpy()\n",
    "          \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image}: {e}\")\n",
    "            return None\n",
    "    def download_image_from_url(self,url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {e}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1123d8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: enroll inorchestrating workflows for genai applications dear friends, a good way to get started in ai is to start with coursework, which gives a systematic way to gain knowledge, and then to work on projects. for many who hear this advice, ‚Äú projects ‚Äù may evoke a significant undertaking that delivers value to users. but i encourage you to set a lower bar and relish small, weekend tinkering projects that let you learn, even if they don ‚Äô t result in a meaningful deliverable. recently, my son and daughter ( ages 3 and 5 ) were building lego vehicles. they built a beautiful ice - cream truck as well as a... umm... colorful and asymmetric dinosaur car, shown in the picture above. while most observers would judge the ice - cream truck as the superior creation, my kids built it by following lego ‚Äô sinstructions, and it is likely identical to thousands of ice - cream trucks built by others. in contrast, building the dinosaur car required creativity and novel thinking. the exercise helped them hone their ability to pick and assemble lego building blocks. there is, of course, room for both mimicking others ‚Äô designs ( with permission ) and coming up with your own. as a \n",
      " Embedding shape: (1, 1024)\n",
      "Chunk: Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build \n",
      " Embedding shape: (1, 1024)\n",
      "Chunk: Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: sure a commercial provider will soon, if they haven ‚Äô t already, build a better product than i was able to throw together in a couple of hours on a weekend. but such tinkering helps me hone my intuition and master software components ( i now know how to programmatically interface with google docs ) that might be useful in future projects. if you have an idea for a project, i encourage you to build it! often, working on a project will also help you decide what additional skills to learn, perhaps through coursework. to sustain momentum, it helps to find friends with whom to talk about ideas and celebrate projects ‚Äî large or small. keep tinkering! andrew stay updated with weekly ai news and insights delivered to your inbox \n",
      " Embedding shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scraped_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_articles = json.load(f)\n",
    "\n",
    "textP = TextPreprocessor()\n",
    "\n",
    "\n",
    "\n",
    "# Chunk —ñ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º\n",
    "chunks = TextPreprocessor.chunk_text_with_title(scraped_articles[0],tokenizer=textP.clip_processor.tokenizer)\n",
    "\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "    chunk = textP.clean_text(chunk)\n",
    "    embedding = textP.get_clip_text_embedding(chunk)\n",
    "    print(f\"Chunk: {chunk} \\n Embedding shape: {embedding.shape}\")\n",
    "    embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20caac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 1024)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from IPython.display import display\n",
    "preprocessor = ImagePreprocessor()\n",
    "\n",
    "# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è\n",
    "image_url = \"https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2Funnamed---2024-05-22T145628.272-2.png&w=3840&q=75\"  # –∑–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ –≤–∞—à URL\n",
    "image = preprocessor.download_image_from_url(image_url)\n",
    "img_embedding = None\n",
    "if image:\n",
    "    preprocessor = ImagePreprocessor()\n",
    "    result = preprocessor.process_image(image)\n",
    "    if result.any():\n",
    "        print(\"Embedding shape:\", result.shape)\n",
    "        img_embedding = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1279a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between query and image embedding: 0.3655830919742584\n",
      "Chunk 0 cosine similarity with image embedding: 0.2615649104118347\n",
      "Chunk 1 cosine similarity with image embedding: 0.38722503185272217\n",
      "Chunk 2 cosine similarity with image embedding: 0.3290191888809204\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "query_embedding = textP.get_clip_text_embedding(\"tell something about dinosaurs and lego cars and ai \")\n",
    "query_embedding_tensor = torch.tensor(query_embedding)  # —è–∫—â–æ numpy ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ —Ç–µ–Ω–∑–æ—Ä\n",
    "query_embedding_norm = torch.nn.functional.normalize(query_embedding_tensor, p=2, dim=1)\n",
    "# –ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, img_embedding ‚Äî numpy –º–∞—Å–∏–≤ –∞–±–æ —Ç–µ–Ω–∑–æ—Ä —Ä–æ–∑–º—ñ—Ä–æ–º [1, 512]\n",
    "img_embedding_tensor = torch.tensor(img_embedding)  # —è–∫—â–æ numpy ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ —Ç–µ–Ω–∑–æ—Ä\n",
    "img_embedding_norm = torch.nn.functional.normalize(img_embedding_tensor, p=2, dim=1)\n",
    "print(f\"similarity between query and image embedding: {torch.nn.functional.cosine_similarity(query_embedding_norm, img_embedding_norm).item()}\")\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    embedding_tensor = torch.tensor(embedding)\n",
    "    embedding_norm = torch.nn.functional.normalize(embedding_tensor, p=2, dim=1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding_norm, img_embedding_norm)\n",
    "    print(f\"Chunk {i} cosine similarity with image embedding: {cos_sim.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6088275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 cosine similarity with image embedding: 1.0000001192092896\n",
      "Chunk 1 cosine similarity with image embedding: 0.7452438473701477\n",
      "Chunk 2 cosine similarity with image embedding: 0.8404936790466309\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# –ü—Ä–∏–ø—É—Å—Ç–∏–º–æ, img_embedding ‚Äî numpy –º–∞—Å–∏–≤ –∞–±–æ —Ç–µ–Ω–∑–æ—Ä —Ä–æ–∑–º—ñ—Ä–æ–º [1, 512]\n",
    "img_embedding_tensor = torch.tensor(img_embedding)  # —è–∫—â–æ numpy ‚Äî –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ —Ç–µ–Ω–∑–æ—Ä\n",
    "img_embedding_norm = torch.nn.functional.normalize(img_embedding_tensor, p=2, dim=1)\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "\n",
    "    embedding_tensor_i = torch.tensor(embeddings[i])\n",
    "    embedding_norm_i = torch.nn.functional.normalize(embedding_tensor_i, p=2, dim=1)\n",
    "    embedding_tensor = torch.tensor(embeddings[0])\n",
    "    embedding_norm = torch.nn.functional.normalize(embedding_tensor, p=2, dim=1)\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding_norm, embedding_norm_i)\n",
    "    print(f\"Chunk {i} cosine similarity with image embedding: {cos_sim.item()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23aff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É\n",
    "with open(\"scraped_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scraped_articles = json.load(f)\n",
    "\n",
    "# –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è ‚Äî –∑–∞–ª–∏—à–∞—î–º–æ –ª–∏—à–µ —Ç—ñ, –¥–µ title –Ω–µ –¥–æ—Ä—ñ–≤–Ω—é—î \"Subscribe to The Batch\"\n",
    "scraped_articles = [article for article in scraped_articles if article.get(\"title\") != \"Subscribe to The Batch\"]\n",
    "\n",
    "# (–û–ø—Ü—ñ–π–Ω–æ) –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ—á–∏—â–µ–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É –Ω–∞–∑–∞–¥ —É —Ñ–∞–π–ª\n",
    "with open(\"scraped_articles_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scraped_articles, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572c6e5",
   "metadata": {},
   "source": [
    "## vectorized store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05b23e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "import json\n",
    "import uuid\n",
    "import torch\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "class MultimodalDB:\n",
    "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
    "        \"\"\"Initialize the MultimodalDB with text and image processors and vector store\"\"\"\n",
    "        self.text_processor = TextPreprocessor()\n",
    "        self.image_processor = ImagePreprocessor()\n",
    "        self.client = chromadb.PersistentClient(path=persist_directory)\n",
    "        \n",
    "        # Create collections for text and image embeddings\n",
    "        self.text_collection = self.client.get_or_create_collection(\n",
    "            name=\"text_embeddings\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        self.image_collection = self.client.get_or_create_collection(\n",
    "            name=\"image_embeddings\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "\n",
    "    def process_article(self, article: Dict[str, Any]) -> Tuple[List[np.ndarray], List[np.ndarray], Dict[str, Any]]:\n",
    "        \"\"\"Process a single article to extract text and image embeddings\"\"\"\n",
    "        # Process text\n",
    "        text_embeddings = []\n",
    "        chunks = self.text_processor.chunk_text_with_title(\n",
    "            article, \n",
    "            tokenizer=self.text_processor.clip_processor.tokenizer\n",
    "        )\n",
    "        \n",
    "        text_embeddings = []\n",
    "        valid_chunks = []\n",
    "        for chunk in chunks:\n",
    "            chunk = self.text_processor.clean_text(chunk)\n",
    "            embedding = self.text_processor.get_clip_text_embedding(chunk)\n",
    "            if embedding is not None:\n",
    "                text_embeddings.append(embedding)\n",
    "                valid_chunks.append(chunk)\n",
    "\n",
    "        # Process images\n",
    "        image_embeddings = []\n",
    "        for img_data in article.get('images', []):\n",
    "            image = self.image_processor.download_image_from_url(img_data['url'])\n",
    "            if image:\n",
    "                embedding = self.image_processor.process_image(image)\n",
    "                if embedding is not None:\n",
    "                    image_embeddings.append(embedding)\n",
    "                    img_data['processed'] = True\n",
    "                else:\n",
    "                    img_data['processed'] = False\n",
    "            else:\n",
    "                img_data['processed'] = False\n",
    "\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'article_url': article.get('url', ''),\n",
    "            'title': article.get('title', ''),\n",
    "            'publication_date': article.get('publication_date', ''),\n",
    "            'processed_at': article.get('scraped_at', '')\n",
    "        }\n",
    "\n",
    "        return text_embeddings, image_embeddings, metadata,valid_chunks\n",
    "\n",
    "    def add_article(self, article: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Add an article to the database, processing both text and images\"\"\"\n",
    "        text_embeddings, image_embeddings, metadata,chunks = self.process_article(article)\n",
    "        \n",
    "        # Add text embeddings\n",
    "        text_ids = []\n",
    "        for i, embedding in enumerate(text_embeddings):\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            chunk_metadata = {\n",
    "                **metadata,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(text_embeddings),\n",
    "                'chunk_text': chunks[i] if i < len(chunks) else ''\n",
    "            }\n",
    "            \n",
    "            self.text_collection.add(\n",
    "                embeddings=[embedding.flatten().tolist()],\n",
    "                metadatas=[chunk_metadata],\n",
    "                ids=[chunk_id],\n",
    "                documents=[chunks[i]] \n",
    "            )\n",
    "            text_ids.append(chunk_id)\n",
    "\n",
    "        # Add image embeddings\n",
    "        image_ids = []\n",
    "        for i, embedding in enumerate(image_embeddings):\n",
    "            image_id = str(uuid.uuid4())\n",
    "            image_metadata = {\n",
    "                **metadata,\n",
    "                'image_index': i,\n",
    "                'image_url': article['images'][i]['url'] if i < len(article['images']) else '',\n",
    "                'total_images': len(image_embeddings)\n",
    "            }\n",
    "            \n",
    "            self.image_collection.add(\n",
    "                embeddings=[embedding.flatten().tolist()],\n",
    "                metadatas=[image_metadata],\n",
    "                ids=[image_id]\n",
    "            )\n",
    "            image_ids.append(image_id)\n",
    "\n",
    "        return {\n",
    "            'text_ids': text_ids,\n",
    "            'image_ids': image_ids\n",
    "        }\n",
    "\n",
    "    def search(self, query: str, n_results: int = 5, modality: str = 'both') -> Dict[str, Any]:\n",
    "        \"\"\"Search the database using a text query\"\"\"\n",
    "        # Get query embedding\n",
    "        query_text_embedding = self.text_processor.get_clip_text_embedding(query)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if modality in ['text', 'both']:\n",
    "            # Search text collection\n",
    "            text_results = self.text_collection.query(\n",
    "                query_embeddings=[query_text_embedding.flatten().tolist()],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            results['text_results'] = text_results\n",
    "        \n",
    "        if modality in ['image', 'both']:\n",
    "            # Search image collection using the same text embedding\n",
    "            image_results = self.image_collection.query(\n",
    "                query_embeddings=[query_text_embedding.flatten().tolist()],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            results['image_results'] = image_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def add_articles_batch(self, articles: List[Dict[str, Any]]) -> List[Dict[str, List[str]]]:\n",
    "        \"\"\"Process and add multiple articles in batch\"\"\"\n",
    "        results = []\n",
    "        for article in articles:\n",
    "            try:\n",
    "                article_ids = self.add_article(article)\n",
    "                results.append(article_ids)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {article.get('title', 'Unknown')}: {e}\")\n",
    "                results.append({'text_ids': [], 'image_ids': []})\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MultimodalDB\n",
    "db = MultimodalDB(persist_directory=\"./multimodal_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708d5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing articles...\n",
      "Processed 50 articles\n",
      "\n",
      "Text Results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'article_url': 'https://www.deeplearning.ai/the-batch/issue-292',\n",
       "  'chunk_index': 0,\n",
       "  'processed_at': '2025-06-13 02:07:08.680546',\n",
       "  'publication_date': 'Mar 12, 2025',\n",
       "  'title': 'Judge Upholds Copyright in AI Training Case',\n",
       "  'total_chunks': 2},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-292',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_text': \"Title: Judge Upholds Copyright in AI Training Case Content: a united states court delivered a major ruling that begins to answer the question whether, and under what conditions, training an ai system on copyrighted material is considered fair use that doesn ‚Äô t require permission. what ‚Äô s new : a u. s. circuit judgeruledon a claim by the legal publisher thomson reuters that ross intelligence, an ai - powered legal research service, could not claim that training its ai system on materials owned by thomson reuters was a so - called ‚Äú fair use.‚Äù training the system did not qualify as fair use, he decided, because its output competed with thomson reuters ‚Äô publications. how it works : thomson reuters hadsuedross intelligence after the defendant trained an ai model using 2, 2 4 3 works produced by thomson reuters without the latter ‚Äô s permission. this ruling reversed an earlier decision in 2 0 2 3, when the same judge had allowed ross intelligence ‚Äô s fair - use defense to proceed to trial. in the new ruling, he found that ross intelligence ‚Äô s use failed to meet the definition of fair use in key respects. ( a jury trial is scheduled to determine whether thomson reuters'copyright was in effect at the time of the infringement and other aspects of the case.) behind the\",\n",
       "  'processed_at': '2025-06-13 02:07:08.680546',\n",
       "  'publication_date': 'Mar 12, 2025',\n",
       "  'title': 'Judge Upholds Copyright in AI Training Case',\n",
       "  'total_chunks': 2},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-292',\n",
       "  'chunk_index': 0,\n",
       "  'chunk_text': \"Title: Judge Upholds Copyright in AI Training Case Content: a united states court delivered a major ruling that begins to answer the question whether, and under what conditions, training an ai system on copyrighted material is considered fair use that doesn ‚Äô t require permission. what ‚Äô s new : a u. s. circuit judgeruledon a claim by the legal publisher thomson reuters that ross intelligence, an ai - powered legal research service, could not claim that training its ai system on materials owned by thomson reuters was a so - called ‚Äú fair use.‚Äù training the system did not qualify as fair use, he decided, because its output competed with thomson reuters ‚Äô publications. how it works : thomson reuters hadsuedross intelligence after the defendant trained an ai model using 2, 2 4 3 works produced by thomson reuters without the latter ‚Äô s permission. this ruling reversed an earlier decision in 2 0 2 3, when the same judge had allowed ross intelligence ‚Äô s fair - use defense to proceed to trial. in the new ruling, he found that ross intelligence ‚Äô s use failed to meet the definition of fair use in key respects. ( a jury trial is scheduled to determine whether thomson reuters'copyright was in effect at the time of the infringement and other aspects of the case.) behind the\",\n",
       "  'processed_at': '2025-06-13 02:07:08.680546',\n",
       "  'publication_date': 'Mar 12, 2025',\n",
       "  'title': 'Judge Upholds Copyright in AI Training Case',\n",
       "  'total_chunks': 2}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'article_url': 'https://www.deeplearning.ai/the-batch/issue-xi',\n",
       "  'image_index': 0,\n",
       "  'processed_at': '2025-06-13 02:07:35.338519',\n",
       "  'publication_date': 'Jun 26, 2019',\n",
       "  'title': 'Between Consenting Electrons',\n",
       "  'total_images': 1},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-xi',\n",
       "  'image_index': 0,\n",
       "  'processed_at': '2025-06-13 02:07:35.338519',\n",
       "  'publication_date': 'Jun 26, 2019',\n",
       "  'title': 'Between Consenting Electrons',\n",
       "  'total_images': 1},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-xi',\n",
       "  'image_index': 0,\n",
       "  'image_url': 'https://dl-staging-website.ghost.io/content/images/2022/09/c55611c3-6f6f-4ef2-844d-ae193d57cfc4.png',\n",
       "  'processed_at': '2025-06-13 02:07:35.338519',\n",
       "  'publication_date': 'Jun 26, 2019',\n",
       "  'title': 'Between Consenting Electrons',\n",
       "  'total_images': 1}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the cleaned articles\n",
    "with open(\"scraped_articles_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Process articles in batch\n",
    "print(\"Processing articles...\")\n",
    "results = db.add_articles_batch(articles[:50])  # Start with first 5 articles as a test\n",
    "print(f\"Processed {len(results)} articles\")\n",
    "\n",
    "# Try a search query\n",
    "query = \"AI and machine learning applications\"\n",
    "search_results = db.search(query, n_results=3)\n",
    "\n",
    "print(\"\\nText Results:\")\n",
    "for i, (doc_id, metadata) in enumerate(zip(search_results['text_results']['ids'], search_results['text_results']['metadatas'])):\n",
    "   display(metadata)\n",
    "print(\"\\nImage Results:\")\n",
    "for i, (doc_id, metadata) in enumerate(zip(search_results['image_results']['ids'], search_results['image_results']['metadatas'])):\n",
    "    display(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3f368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a search query\n",
    "db = MultimodalDB(persist_directory=\"./multimodal_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c5282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs',\n",
       "  'chunk_index': 1,\n",
       "  'processed_at': '2025-06-13 02:06:44.103029',\n",
       "  'publication_date': 'May 22, 2024',\n",
       "  'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.',\n",
       "  'total_chunks': 3},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs',\n",
       "  'chunk_index': 1,\n",
       "  'chunk_text': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build',\n",
       "  'processed_at': '2025-06-13 02:06:44.103029',\n",
       "  'publication_date': 'May 22, 2024',\n",
       "  'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.',\n",
       "  'total_chunks': 3},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs',\n",
       "  'chunk_index': 1,\n",
       "  'chunk_text': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build',\n",
       "  'processed_at': '2025-06-13 02:06:44.103029',\n",
       "  'publication_date': 'May 22, 2024',\n",
       "  'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.',\n",
       "  'total_chunks': 3}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'article_url': 'https://www.deeplearning.ai/the-batch/issue-230',\n",
       "  'image_index': 0,\n",
       "  'image_url': 'https://dl-staging-website.ghost.io/content/images/2024/01/EVILGPT_1200px-1.gif',\n",
       "  'processed_at': '2025-06-13 02:07:49.940877',\n",
       "  'publication_date': 'Jan 3, 2024',\n",
       "  'title': 'GPT-4 Wouldn‚Äôt Lie to Me . . . Would It?',\n",
       "  'total_images': 1},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-230',\n",
       "  'image_index': 0,\n",
       "  'processed_at': '2025-06-13 02:07:49.940877',\n",
       "  'publication_date': 'Jan 3, 2024',\n",
       "  'title': 'GPT-4 Wouldn‚Äôt Lie to Me . . . Would It?',\n",
       "  'total_images': 1},\n",
       " {'article_url': 'https://www.deeplearning.ai/the-batch/issue-230',\n",
       "  'image_index': 0,\n",
       "  'processed_at': '2025-06-13 02:07:49.940877',\n",
       "  'publication_date': 'Jan 3, 2024',\n",
       "  'title': 'GPT-4 Wouldn‚Äôt Lie to Me . . . Would It?',\n",
       "  'total_images': 1}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "query = \"tiny toy dinosaur on lego car\" \n",
    "search_results = db.search(query, n_results=3)\n",
    "\n",
    "print(\"\\nText Results:\")\n",
    "for i, (doc_id, metadata) in enumerate(zip(search_results['text_results']['ids'], search_results['text_results']['metadatas'])):\n",
    "   display(metadata)\n",
    "print(\"\\nImage Results:\")\n",
    "for i, (doc_id, metadata) in enumerate(zip(search_results['image_results']['ids'], search_results['image_results']['metadatas'])):\n",
    "    display(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1dcf04",
   "metadata": {},
   "source": [
    "## retrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe7c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class MultimodalRetriever:\n",
    "    def __init__(self, vector_store: MultimodalDB):\n",
    "        self.vector_store = vector_store\n",
    "    \n",
    "    def retrieve(self, query: str, n_results: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant content for a given query\"\"\"\n",
    "        \n",
    "        # Perform hybrid search\n",
    "        search_results = self.vector_store.search(query, n_results)\n",
    "        with open(\"retrieval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(search_results, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Process and rank results\n",
    "        processed_results = self.process_search_results(search_results, query, n_results)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def process_search_results(self, search_results: Dict, query: str,n_results:int) -> Dict[str, Any]:\n",
    "        \"\"\"Process and structure search results\"\"\"\n",
    "        \n",
    "        text_results = search_results.get('text_results', {})\n",
    "        image_results = search_results.get('image_results', {})\n",
    "        \n",
    "        # Structure text results\n",
    "        text_chunks = []\n",
    "        if text_results.get('documents'):\n",
    "            for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                text_results['documents'][0],\n",
    "                text_results['metadatas'][0],\n",
    "                text_results['distances'][0]\n",
    "            )):\n",
    "                text_chunks.append({\n",
    "                    'content': doc,\n",
    "                    'metadata': metadata,\n",
    "                    'relevance_score': 1 - distance,  # Convert distance to similarity\n",
    "                    'type': 'text'\n",
    "                })\n",
    "        \n",
    "        # Structure image results\n",
    "        image_items = []\n",
    "        if image_results.get('metadatas'):\n",
    "            for metadata, distance in zip(\n",
    "                image_results['metadatas'][0],\n",
    "                image_results['distances'][0]\n",
    "            ):\n",
    "                image_items.append({\n",
    "                    'metadata': metadata,\n",
    "                    'relevance_score': 1 - distance,\n",
    "                    'type': 'image'\n",
    "                })\n",
    "        \n",
    "        # Combine and sort by relevance\n",
    "        all_results = text_chunks + image_items\n",
    "        all_results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'results': all_results[:n_results],\n",
    "            'total_found': len(all_results)\n",
    "        }\n",
    "    \n",
    "    def get_context_for_llm(self, retrieval_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format retrieval results as context for LLM\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        context_parts.append(f\"Query: {retrieval_results['query']}\\n\")\n",
    "        context_parts.append(\"Relevant Information:\\n\")\n",
    "        \n",
    "        for i, result in enumerate(retrieval_results['results'][:5], 1):\n",
    "            if result['type'] == 'text':\n",
    "                context_parts.append(f\"{i}. {result['content']}\")\n",
    "                if 'title' in result['metadata']:\n",
    "                    context_parts.append(f\"   Source: {result['metadata']['title']}\")\n",
    "            elif result['type'] == 'image':\n",
    "                context_parts.append(f\"{i}. Image: {result['metadata'].get('description', 'No description')}\")\n",
    "                if 'alt' in result['metadata']:\n",
    "                    context_parts.append(f\"   Alt text: {result['metadata']['alt']}\")\n",
    "                    \n",
    "            context_parts.append(\"\")  # Empty line for separation\n",
    "        \n",
    "        return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af426200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Results:\n",
      "{'query': 'tell something about dinosaurs and lego cars and ai ', 'results': [{'content': None, 'metadata': {'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs', 'chunk_index': 1, 'processed_at': '2025-06-13 02:06:44.103029', 'publication_date': 'May 22, 2024', 'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.', 'total_chunks': 3}, 'relevance_score': 0.6465099453926086, 'type': 'text'}, {'content': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build', 'metadata': {'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs', 'chunk_index': 1, 'chunk_text': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build', 'processed_at': '2025-06-13 02:06:44.103029', 'publication_date': 'May 22, 2024', 'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.', 'total_chunks': 3}, 'relevance_score': 0.6465099453926086, 'type': 'text'}, {'content': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build', 'metadata': {'article_url': 'https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs', 'chunk_index': 1, 'chunk_text': 'Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build', 'processed_at': '2025-06-13 02:06:44.103029', 'publication_date': 'May 22, 2024', 'title': 'Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.', 'total_chunks': 3}, 'relevance_score': 0.6465099453926086, 'type': 'text'}], 'total_found': 6}\n",
      "\n",
      "Context for LLM:\n",
      "Query: tell something about dinosaurs and lego cars and ai \n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "1. None\n",
      "   Source: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.\n",
      "\n",
      "2. Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build\n",
      "   Source: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.\n",
      "\n",
      "3. Title: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative. Content: ‚Äô designs ( with permission ) and coming up with your own. as a parent, i try to celebrate both. ( to be honest, i celebrated the dinosaur car more.) when learning to build lego, it ‚Äô s helpful to start by following a template. but eventually, building your own unique projects enriches your skills. as a developer, too, i try to celebrate unique creations. yes, it is nice to have beautiful software, and the impact of the output does matter. but good software is often written by people who spend many hours tinkering and building things. by building unique projects, you master key software building blocks. then, using those blocks, you can go on to build bigger projects. i routinely tinker with building ai applications, and a lot of my tinkering doesn ‚Äô t result in anything useful. my latest example : i built a streamlit app that would authenticate to google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. i didn ‚Äô t find it useful in the end because of friction in the user interface, and i ‚Äô m sure a commercial provider will soon, if they haven ‚Äô t already, build\n",
      "   Source: Project Idea ‚Äî A Car for DinosaursAI projects don‚Äôt need to have a meaningful deliverable. Lower the bar and do something creative.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrive= MultimodalRetriever(vector_store=db)\n",
    "query= \"tell something about dinosaurs and lego cars and ai \"\n",
    "retrivial=retrive.retrieve(query, n_results=3)\n",
    "print(\"Retrieval Results:\")\n",
    "\n",
    "print(retrivial)\n",
    "context=retrive.get_context_for_llm(retrivial)\n",
    "print(\"\\nContext for LLM:\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619337d3",
   "metadata": {},
   "source": [
    "## llm integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class LLMIntegration:\n",
    "    def __init__(self, provider=\"openai\", model=\"gpt-3.5-turbo\"):\n",
    "        load_dotenv()\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        \n",
    "        if provider == \"openai\":\n",
    "            self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        elif provider == \"anthropic\":\n",
    "            self.anthropic = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    def generate_response(self, query: str, context: str, max_tokens: int = 500) -> str:\n",
    "        \"\"\"Generate response using LLM\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are an AI assistant that helps users find information from The Batch news articles. \n",
    "        Use the provided context to answer questions accurately. If the context doesn't contain enough information \n",
    "        to answer the question, say so clearly. Always cite which articles or sources you're referencing.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "        Context from The Batch articles:\n",
    "        {context}\n",
    "        \n",
    "        User Question: {query}\n",
    "        \n",
    "        Please provide a comprehensive answer based on the context above.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.provider == \"openai\":\n",
    "            return self._generate_openai_response(system_prompt, user_prompt, max_tokens)\n",
    "        elif self.provider == \"anthropic\":\n",
    "            return self._generate_anthropic_response(system_prompt, user_prompt, max_tokens)\n",
    "        else:\n",
    "            return \"LLM provider not supported.\"\n",
    "    \n",
    "    def _generate_openai_response(self, system_prompt: str, user_prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using OpenAI\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def _generate_anthropic_response(self, system_prompt: str, user_prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Anthropic Claude\"\"\"\n",
    "        try:\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.3,\n",
    "                system=system_prompt,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def summarize_article(self, article_content: str) -> str:\n",
    "        \"\"\"Generate summary of an article\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please provide a concise summary of the following article from The Batch:\n",
    "        \n",
    "        {article_content}\n",
    "        \n",
    "        Summary should be 2-3 sentences highlighting the key points.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.generate_response(\"Summarize this article\", prompt, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= LLMIntegration()\n",
    "llm_response = llm.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f854a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Response:\n",
      "Based on the context provided from The Batch article \"Project Idea ‚Äî A Car for Dinosaurs,\" the article discusses the creative aspect of AI projects and the process of building unique projects using LEGO cars as an analogy. The author emphasizes the importance of celebrating unique creations in both LEGO building and software development. They mention how starting with templates is helpful but building one's own projects enriches skills in both LEGO and software development.\n",
      "\n",
      "Furthermore, the author shares their personal experience of tinkering with AI applications, highlighting that not all projects need to have a meaningful deliverable. They mention an example of building a Streamlit app that interacts with Google Docs using a large language model, even though the project did not result in a useful outcome due to user interface issues.\n",
      "\n",
      "In summary, the article encourages creativity and experimentation in AI projects, drawing parallels between building LEGO cars, celebrating unique creations, and the process of tinkering with AI applications. It emphasizes that projects don't always have to lead to a tangible outcome to be valuable in terms of skill-building and exploration.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLLM Response:\")\n",
    "print(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
